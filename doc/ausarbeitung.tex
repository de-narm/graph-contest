%TODO FIND SOURCE FOR NP HARDNESS

\documentclass[]{llncs}

\usepackage{tikz}
\usepackage{enumitem}
\usepackage{verbatim}
\usepackage{biblatex}
\usepackage{filecontents}
\usepackage[absolute]{textpos}

\usetikzlibrary{positioning}

\begin{filecontents}{bib.bib}
@electronic{bollobas98,
  abstract = {The time has now come when graph theory should be part of the education of every serious student of mathematics and computer science, both for its own sake and to enhance the appreciation of mathematics as a whole. This book is an in-depth account of graph theory, written with such a student in mind; it reflects the current state of the subject and emphasizes connections with other branches of pure mathematics. The volume grew out of the author's earlier book, Graph Theory -- An Introductory Course, but its length is well over twice that of its predecessor, allowing it to reveal many exciting new developments in the subject. Recognizing that graph theory is one of several courses competing for the attention of a student, the book contains extensive descriptive passages designed to convey the flavor of the subject and to arouse interest. In addition to a modern treatment of the classical areas of graph theory such as coloring, matching, extremal theory, and algebraic graph theory, the book presents a detailed account of newer topics, including Szemer'edi's Regularity Lemma and its use, Shelah's extension of the Hales-Jewett Theorem, the precise nature of the phase transition in a random graph process, the connection between electrical networks and random walks on graphs, and the Tutte polynomial and its cousins in knot theory. In no other branch of mathematics is it as vital to tackle and solve challenging exercises in order to master the subject. To this end, the book contains an unusually large number of well thought-out exercises: over 600 in total. Although some are straightforward, most of them are substantial, and others will stretch even the most able reader.},
  added-at = {2015-05-19T09:19:57.000+0200},
  address = {New York},
  author = {Bollob\'{a}s, B\'{e}lla},
  biburl = {https://www.bibsonomy.org/bibtex/27b84cb41ef679da77300486d9f6f0916/ytyoun},
  doi = {10.1007/978-1-4612-0619-4},
  interhash = {875127e4e9980d87bf0bdeab7fa97559},
  intrahash = {7b84cb41ef679da77300486d9f6f0916},
  isbn = {9781461206194},
  keywords = {bollobas characteristic eigenvalues graph.theory hoffman monotonicity polynomial rayleigh textbook},
  publisher = {Springer},
  refid = {682118471},
  timestamp = {2017-11-10T06:33:22.000+0100},
  title = {Modern Graph Theory},
  year = 1998
}

@article{journals/corr/abs-1808-10519,
  added-at = {2018-09-04T00:00:00.000+0200},
  author = {Bekos, Michael A. and Förster, Henry and Geckeler, Christian and Holländer, Lukas and Kaufmann, Michael and Spallek, Amadäus M. and Splett, Jan},
  biburl = {https://www.bibsonomy.org/bibtex/2e13a59afe7afa2da70a3d34ebc7993c1/dblp},
  ee = {http://arxiv.org/abs/1808.10519},
  interhash = {0703b4c6a706aafb35707a22e420381f},
  intrahash = {e13a59afe7afa2da70a3d34ebc7993c1},
  journal = {CoRR},
  keywords = {dblp},
  timestamp = {2018-09-05T11:36:29.000+0200},
  title = {A Heuristic Approach towards Drawings of Graphs with High Crossing Resolution.},
  url = {http://dblp.uni-trier.de/db/journals/corr/corr1808.html#abs-1808-10519},
  volume = {abs/1808.10519},
  year = 2018
}

@inproceedings{conf/gd/DemelDMRW18,
  added-at = {2019-12-17T16:50:25.000+0100},
  author = {Demel, Almut and Dürrschnabel, Dominik and Mchedlidze, Tamara and Radermacher, Marcel and Wulf, Lasse},
  biburl = {https://www.bibsonomy.org/bibtex/24d53ae536166435e13c78aeb2025d574/duerrschnabel},
  booktitle = {Graph Drawing},
  crossref = {conf/gd/2018},
  editor = {Biedl, Therese C. and Kerren, Andreas},
  ee = {https://doi.org/10.1007/978-3-030-04414-5_20},
  interhash = {44c9d5616b9daf6501104053f403cfdc},
  intrahash = {4d53ae536166435e13c78aeb2025d574},
  isbn = {978-3-030-04414-5},
  keywords = {crossing_angle_maximization greedy_heuristik myown},
  pages = {286-299},
  publisher = {Springer},
  series = {Lecture Notes in Computer Science},
  timestamp = {2019-12-17T16:50:25.000+0100},
  title = {A Greedy Heuristic for Crossing-Angle Maximization.},
  url = {http://dblp.uni-trier.de/db/conf/gd/gd2018.html#DemelDMRW18},
  volume = 11282,
  year = 2018
}

\end{filecontents}

\addbibresource{bib.bib}

\begin{document}
    \title{Graph Drawing Contest 2020 \\
           Crossing Minimization with Randomness}
    \author{Sebastian Benner}
    \maketitle
    
    \abstract{Stuff}

    \section{Introduction} 

	The annual Graph Drawing Contest\footnote{http://mozart.diei.unipg.it/gdcontest/contest2020/challenge.html} is an open challenge to design an algorithm for optimized graph drawing. The exact criteria for such a drawing are changed every couple of years, the current ones remain the same as last years challenge. The Live Challenge will contain between five to ten acyclic directed graphs with up to a few thousands nodes each. All resulting layouts must be submitted within one hour of the graphs being handed out.

The main criteria this time around are crossing which ought to be minimal in the resulting drawing. In itself this already poses a NP-hard problem. Additional constraints placed on the drawing are:
\begin{itemize}
	\item Each edge must be a straight upward facing line, meaning the source of each directed edge must be lower than the target.
	\item Each node must be placed upon a grid of given size.
	\item Crossings between a node and an edge are not permitted, as well as overlapping nodes.
\end{itemize}

After all graphs are collected, for each of the original graphs a best drawing is determined with all the other graphs receiving a weighted score based on the difference in crossings. The highest overall score wins the contest. Each team has to bring its own hardware to run their respective algorithm, meaning there is no limitation in terms of tools used and the given time to solve the task can be counterbalanced by more powerful hardware.

During the last couple of years most if not all of the top scoring contestants based their algorithm at least to some part on randomness which will be the basis of this work. The goal of is to find a balance between lightweight calculations for random steps and a more directed approach to randomness.

	\section{Foundation}
	A $graph\;G$ is defined as an ordered pair $(V, E)$ of $vertices\;V$ and $edges\;E$. Edges are unordered pairs of two vertices $\{x,y\}$, said to $join$  them, and therefore $E$ is a subset of $V^{(2)}$. In the special case where edges are ordered pairs $(x,y)$ with $x$ as $source$ and $y$ as $target$ the graph $D$ is called $directed$ graph or $digraph$. We call the number of vertices the $order$ of $G$ and the number of edges the $size$ of $G$. $V(G)$ and $E(G)$ are the sets of vertices and edges of $G$ respectively, $x \in V(G)$ with vertex $x$ can be written as $x \in G$ while $\{x, y\} \in E(G)$ with unordered edge $\{x,y\}$ is written as $\{x,y\} \in G$. 

For a more comprehensive explanation I refer to the book $Modern$ $Graph$ $Theory$~\cite{bollobas98} upon which this notation is based on.

	\section{Prior Work}
	\paragraph{A Heuristic Approach towards Drawings of
Graphs with High Crossing Resolution~\cite{journals/corr/abs-1808-10519}}
	While not aiming at the same goal, this algorithm served as basis for the last years winner. The aim was to create a drawing with largest minimum angle, called the resolution of $G$,  possible. To achieve this the algorithm build and maintained two sets: all nodes and only such nodes that are deemed critical for the current resolution of $G$. Critical are all nodes connected with edges involved in minimal angles. In each iteration either a node from the set of all critical nodes is chosen uniformly or inverse proportionally to its proximity to a critical nodes from the set of all nodes. To determine the new position of a node rays are used which are cast out uniformly distributed in all directions with the best result becoming the new position. Combined with a energy-based base drawing and some tweaks to avoid local minima, the algorithm proved itself and also won its respective year.

    \paragraph{A Greedy Heuristic for Crossing-Angle Maximization~\cite{conf/gd/DemelDMRW18}}
	Another winning contribution to a different year with the same aim as before, an algorithm to create a drawing with the largest possible minimum angle. Starting from a force directed base drawing the algorithm greedily selects the minimal crossing in the current drawing. A random vertex is chosen and displaced within a square around the original position to achieve the maximal local crossing angle. The size of the square is decreased in each iteration.

    \section{Sugiyama Framework}

    \section{Crossing Minimization}

Following the base drawing, the graph drawing algorithm is heavily based on random displacement of nodes. In order to achieve the best results there are three things to balance against each other: Being as efficient as possible to run as many displacements as possible, directing the algorithm enough to minimize steps without any improvement and allowing enough randomness to minimize the risk of local minima. What follows now is a step by step explanation of the final algorithm, afterwards I will go through possible changes and their impact on the overall results.

\medskip
\begin{enumerate}[start=0]
    \item The displacement process is started with a fix number iterations $I$. Each iteration only tests a single new position for a single node.
    \item A node is chosen completely random and the following values are determined:
    \begin{enumerate}
        \item The initial number of node-overlaps is calculated by comparing the node position with every other node.
        \item To arrive at the number of node-crossings the node position is tested against every edge. Iff the node is within the bounding box of an edge, it is tested against a crossing.
        \item Finally edge-crossings are calculated by testing each edge connected with the node against every other edge, again only by testing if their bounding boxes intersect and testing for an crossing afterwards.
    \end{enumerate}
	\item Iff all metrics equal $0$ all other steps are skipped.
    \item To assign a new position $(x,y)$ to the node, its $x$ value is randomized on the entire width of the grid. The $y$ value is only randomized between the highest lower neighbour and the lowest upper neighbour. This assures that each edge will remain upwards facing.
    \item Afterwards each metric is calculated again.
    \begin{enumerate}
        \item Iff both the number of node-crossings and node-overlaps remain the same or improve compared to before, the new position is kept if the same applies to the number of edge-crossings.
        \item Iff both the number of node-crossings and node-overlaps remain the same or improve compared to before, the new position is kept with an increase in edge-crossings up to $n = 4$ with a decreasing chance between $100\%$ and $0\%$ in the first $I / (n + 1)$ steps. This means an increase of $1$ can be kept in the first half, a increase of $2$ in the first third and so on. 
        \item Regardless of the edge-crossings the new position is kept iff the number of node-overlaps is reduced or the number of node-crossings decreases while the node-overlaps stay the same. This priority assures that the resulting drawing does not violate any restrictions.
        \item Otherwise the displacement is reversed.
    \end{enumerate}
\end{enumerate}

\medskip
The algorithm was implemented in C++ using $Open$ $Graph$ $Drawing$ $Framework$\footnote{https://ogdf.uos.de/}. In order to increase the performance of the edge-crossings algorithm all edges were run in parallel using $OpenMP$\footnote{https://www.openmp.org/}. Across all tested configurations OpenMP only cut the runtime in half as all displacements still run in sequence and the average node has not enough edges to further improve runtime.

\begin{figure}
\centering
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|}
	\hline 
	& \#1 & \#2 & \#3 & \#4 & \#5 & \#6 & \#7 & \#8 & \#9 & \#10 & \#11 & \#12 \\
	\hline 
    Sugiyama & 14 & 46* & 17 & 15* & 37* & 412* & 526 & 911 & 134 & 3,327* & 3,039* & 339,951* \\  
	\hline
	100k & 0 & 4 & 6 & 7 & 32 & 103 & 13 & 328 & 49 & 1,885 & 1,736 & 157,391 \\
	\hline
	1m & 0 & 4 & 5 & 4 & 32 & 76 & 8 & 310 & 45 & 1,475 & 1,721 & 139,542 \\
	\hline
	Tübigen-Bus & 0 & 4 & 5 & 4 & 32 & 81 & 0 & 307 & 38 & 1,568 & 1,721 & 147,628 \\
	\hline 
\end{tabular}
\caption{Performance of base layout and algorithm compared to last years winner}
\label{best-res}
\end{figure}

The results from the base drawing, the algorithm with both $100,000$ and $1.5$ million iterations and the winning entry $Tübingen-Bus$ from last years contest\footnote{http://mozart.diei.unipg.it/gdcontest/contest2019/results.html} can be seen in Table~\ref{best-res}. Results marked with an asterisk do not fulfil all requirements placed upon the drawing. Both runs of the presented algorithm finished within the one hour mark on a $Lenovo\;Thinkpad\;T460$, $100,000$ is included nonetheless as most changes presented later on were only tested with this smaller number of iterations. On most instances last years winner is either equivalent or better performing, especially on larger graphs. There are only two instances were this is not the case, one within each of the runs. Since the algorithm is mostly based upon randomness the run with less iterations is actually better than the longer run twice for the smaller graphs.

	\section{Different approaches}
	This section will go through different approaches not included in the final algorithm to further reduce the number of crossings and some of their results.

	The first thing considered was to change the way the algorithm is parallelized in order to improve performance as the algorithm could be described as randomized brute-forcing within certain boundaries. 
	\begin{itemize}
		\item The algorithm could simply work on all input graphs in parallel. However this cannot yield a better performance than being roughly twice as fast since only two graphs of the dataset occupy most of the runtime. However beyond the scope of the contest this would be the best solution for equally large graphs.
		\item Another approach is to run the displacement steps in parallel. In theory this could scale the same way running the graphs in parallel would but without the requirement of equally sized graphs. The downside would be the possibility of introducing erroneous steps. Surprisingly only the upward-facing attribute of the edges posed a problem when moving multiple nodes at once. Fixing this afterwards would both ruin the speed-up and introduce new edge-crossings.
	\end{itemize}
	
\begin{figure}
\centering
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|}
	\hline 
	& \#1 & \#2 & \#3 & \#4 & \#5 & \#6 & \#7 & \#8 & \#9 & \#10 & \#11 & \#12 \\
	\hline 
	CrossWeight & 2 & 5 & 5 & 8 & 32 & 120 & 23 & 362 & 46 & 2,153 & 2,429 & 231,582 \\
	\hline
	1w & 0 & 4 & 5 & 6 & 32 & 133 & 21 & 372 & 44 & 2,144 & 2,514 & 229,974 \\
	\hline
	10w & 0 & 4 & 5 & 4 & 32 & 134 & 7 & 392 & 39 & 2,136 & 2,540 & 232,182 \\
	\hline
	100w & 0 & 4 & 5 & 6 & 32 & 126 & 14 & 393 & 37 & 2,142 & 2,622 & 228,550 \\
	\hline
	10w mix & 0 & 4 & 6 & 6 & 32 & 122 & 24 & 419 & 39 & 2,124 & 2,625 & 230,613 \\
	\hline
	%100k 0 11 6 6 32 130 21 576 82 2826 1728 212714
	%1m 0 7 4 3 32 83 0 327 44 1682 1721 148462
	%1.5m 0 7 5 5 32 92 0 321 36 1668 1721 143898
\end{tabular}
\caption{Performance of different node selection methods}
\label{select-res}
\end{figure}
	Another major part of optimization seemed to be the selection of nodes, which happens completely at random. Table~\ref{select-res} shows the result from different selection methods, each tested with $100,000$ displacement steps on top of the Sugiyama base drawing. In order of the table the following methods were tested:
	\begin{enumerate}
		\item The basis to the algorithms used was to initially calculate all crossings and select the next node weighted by their crossing number. A fully greedy algorithm would be to prone to local minima. $CrossWeight$ simply used the number of crossings as-is and therefore would not select nodes without any crossings. It is included as a baseline but was never expected to outperform the purely random approach. Its major flaw is that recalculating edge-crossings for every node in each step would not be feasible without massively increasing the runtime, so only the current node was updated. This commonly lead to local minima as most nodes would not be selectable despite being involved in crossings introduced by moving other nodes but never being updated themselves.
		\item $1w$ and all other entries with this naming scheme introduced a base chance for each node to be selected on top of their crossing number. Already with a base weight of $1$ the results are much closer to the purely random approach. As can be seen increasing the weight further improves the results slightly. The problem is how well it improves. A weight of $100$ for most graphs is well beyond the number of crossings a single node could be involved with. Yet despite being almost random, the smaller graphs still perform worse than before. Increasing the weight further would just imitate the random selection but with a large chunk of initial runtime to get the edge-crossings of each node. The results could be improved by updating all nodes in each step, but this would almost multiply the runtime of each step by the number of nodes.
		\item Looking closely the weighted selected outperformed always outperformed random selection on the final and largest graph, but was underperforming in graphs with only a few crossings left. This data called for a mixed approach. The best result was achieved with a base weight of $10$ and a linearly decreasing chance to use the weighted selected from $100\%$ in the first step down to $0\%$ in the last step. However none of the results warranted the extra runtime for a weighted selection.
	\end{enumerate}
	Other tested weighted selection methods included decreasing weights over time - both prioritising nodes with more crossings and nodes not selected for the most steps, periodically updating all edge-crossings every x steps as well as both a changing and static chance to for a purely random and greedy step. Compared to the more in-depth explained ones above they performed worse in either runtime or resulting edge-crossings.

	As no better selection algorithm could be found the next thing to search for was a better way to select the new node position in each step. Once again several different methods were tested with $100,000$ steps on top of the Sugiyama base drawing.
	\begin{itemize}
		\item What first comes to mind would be limiting the x dimension just like the y dimension is limited. By reducing the maximum distance a node could be moved at once the chance for attempts
	\end{itemize}

    \section{Conclusion}

    \printbibliography
\end{document}
